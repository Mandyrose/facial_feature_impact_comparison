[DATASET]
filters=["phase_perc_size"]
image_size = 256
net_input_size = 224
transforms_type = mtcnn
dataset_means = [0.5, 0.5, 0.5]
dataset_stds = [0.5, 0.5, 0.5]
# set this dir to the dataset dir
# raw_dataset_path = /home/ssd_storage/datasets/Cognitive_exp/adva_celeb_finetune_mtcnn/images_for_finetuning
raw_dataset_path = /home/ssd_storage/datasets/processed/1000_ids_num_changed/1000_ids_300_train_50_val_no_asians
crop_scale={"max": 1.0, "min": 1.0}
processed_dataset_root = /home/ssd_storage/datasets/processed
class_filter_dataset_dir = phase_perc_size
# dataset_name = adva_familiar
dataset_name = mandy_faces
# phase_size_dict = {"train": 0.75, "val": 0.25}
phase_size_dict = {"train":300, "val": 50}


# dataset_size_filter_dir = num_classes
#missing 5 with 200 and 300
num_ids = [2,5,10,50, 100]
num_pics = [1,5,10,20,50,100,200,300]




[GENERAL]
root_dir = /home/ssd_storage/experiments
#change to name of experiment (the output folder will be created accordingly)
# experiment_name = dp_sr
# run_name = all_layers_rdms
base_experiment_name = small_ids_no_asians_2_ids_2_1_0
#small_ids_no_asians
experiment_name = small_ids_no_asians

[MODELLING]
feature_parallelized_architectures = ["VGG", "vgg11", "vgg11_bn", "vgg13", "vgg13_bn", "vgg16", "vgg16_bn",
    "vgg19_bn", "vgg19", "AlexNet", "alexnet"]
architecture = vgg16
# If you want to start from the middle of training, set this to the epoch you wish to start from (it will load start_epoch-1 from the dir)
start_epoch = 120
end_epoch = 120
is_pretrained = False
num_classes = 4
#8749
criterion_name = CrossEntropyLoss
criterion_params = {}
batch_size=1
workers=4
performance_test=LFW_TEST
perf_threshold=1.0
#each #num_epochs_to_test we make a LFW test
num_epochs_to_test=10
# num_epochs_to_test = 120
num_batches_per_epoch_limit=1000
logs_path=${GENERAL:root_dir}/${GENERAL:experiment_name}/${MODELLING:architecture}/results/mandy_run/
# checkpoint_path=/home/administrator/experiments/familiarity/pretraining/vgg16/models/119.pth
checkpoint_path=/home/hdd_storage/MR/results/asians/100_ids/small_ids_no_asians_100_ids_100_300_7/vgg16/models/119.pth

[OPTIMIZING]
optimizer = Adam
optimizer_params = {
    "lr": 1e-5}
lr_scheduler = ReduceLROnPlateau
lr_scheduler_params = {
    "factor": 0.1}

[LFW_TEST]
# reps_layers=BlauchEquivalentExtractor
reps_layers=Fc78Dict
labeled_pairs_path=/home/administrator/experiments/familiarity/dataset/image_pairs_lists/mutualy_exclusive/pretraining_verification.txt
reps_cache_path=${GENERAL:root_dir}/${GENERAL:experiment_name}/lfw/reps/
comparison_metric=cos
lfw_dir=/home/administrator/experiments/familiarity/dataset/processed_pretraining_dataset/phase_perc_size/pretraining_fixed_{'train': 0.7, 'val': 0.2, 'test': 0.1}/test
reps_results_path=${GENERAL:root_dir}/${GENERAL:experiment_name}/${MODELLING:architecture}/results/mandy_run/
output_filename=verification_fc8.csv
# pretraining_verification_performance.csv

[REP_BEHAVIOUR]
dist_mat=True
reps_layers=BlauchEquivalentExtractor
datasets = {
                "mat_dom": "/home/ssd_storage/datasets/dp_sr_comparison/dp_sr_images_MTCNN/"
                
            }
            # "mat_dom": "/home/ssd_storage/datasets/dp_sr_comparison/dp_sr_images_MTCNN/'matt and domonic jpg'",
            #     "omer_eyal": "/home/ssd_storage/datasets/dp_sr_comparison/dp_sr_images_MTCNN/'matt and domonic jpeg'",
            #     "tamar_marina": "/home/ssd_storage/datasets/dp_sr_comparison/dp_sr_images_MTCNN/'matt and domonic jpeg'",
            #     "tazmin_tess": "/home/ssd_storage/datasets/dp_sr_comparison/dp_sr_images_MTCNN/'matt and domonic jpeg'"
comparison_metric=cos
output_filename=dist_mat
reps_cache_path=${GENERAL:root_dir}/${GENERAL:experiment_name}/pairs/reps/mandy_run/
reps_results_path=${GENERAL:root_dir}/${GENERAL:experiment_name}/${MODELLING:architecture}/results/mandy_run/
# /home/ssd_storage/experiments/dp_sr/vgg16/results/